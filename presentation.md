# Yerel LLM ile DokÃ¼man Soru-Cevap Servisi
## Teknik Sunum

---

# 1. Problem TanÄ±mÄ±

## KullanÄ±cÄ± Ä°htiyacÄ±

GÃ¼nÃ¼mÃ¼zde birÃ§ok kurum ve birey, bÃ¼yÃ¼k miktarda dokÃ¼mana sahip olmasÄ±na raÄŸmen bu dokÃ¼manlardan hÄ±zlÄ±ca bilgi Ã§Ä±karmak konusunda zorluk yaÅŸamaktadÄ±r.

**Temel Sorunlar:**
- ğŸ“š DokÃ¼manlar arasÄ±nda manuel arama zaman alÄ±cÄ±
- ğŸ”’ Hassas veriler bulut servislerine yÃ¼klenemez
- ğŸ’° Ticari API'ler maliyet oluÅŸturur
- ğŸŒ Ä°nternet baÄŸlantÄ±sÄ± gerektiren Ã§Ã¶zÃ¼mler her zaman kullanÄ±lamaz

## Ã‡Ã¶zÃ¼m YaklaÅŸÄ±mÄ±

Bu servis, **RAG (Retrieval-Augmented Generation)** yaklaÅŸÄ±mÄ±nÄ± kullanarak:

1. **Yerel Ã‡alÄ±ÅŸma**: TÃ¼m iÅŸlemler kullanÄ±cÄ±nÄ±n bilgisayarÄ±nda gerÃ§ekleÅŸir
2. **Semantik Arama**: Anahtar kelime yerine anlam bazlÄ± arama
3. **BaÄŸlam Destekli Cevap**: LLM, sadece ilgili dokÃ¼man parÃ§alarÄ±nÄ± kullanarak cevap Ã¼retir
4. **Kolay KullanÄ±m**: Web arayÃ¼zÃ¼ ile teknik bilgi gerektirmeden kullanÄ±m

---

# 2. LLM ile Ä°letiÅŸim KatmanÄ±

## Teknoloji SeÃ§imi: Ollama

| Alternatif | AvantajlarÄ± | DezavantajlarÄ± |
|------------|-------------|----------------|
| **Ollama** âœ“ | Kolay kurulum, OpenAI uyumlu API | Model boyutlarÄ± bÃ¼yÃ¼k |
| LM Studio | GUI desteÄŸi | API kÄ±sÄ±tlÄ± |
| HuggingFace Transformers | Esneklik | Kurulum karmaÅŸÄ±k |

**Ollama Tercihi Nedenleri:**
- Tek komutla model indirme
- REST API ile kolay entegrasyon
- DÃ¼ÅŸÃ¼k bellek optimizasyonlarÄ±

## ModÃ¼l YapÄ±sÄ±

```python
# app/services/llm_service.py
class LLMService:
    def check_connection(self) -> bool
    def is_model_available(self) -> bool
    def generate_response(question, context) -> str
```

## Prompt MÃ¼hendisliÄŸi

RAG iÃ§in optimize edilmiÅŸ sistem prompt'u:
- BaÄŸlam bilgisini zorunlu kÄ±lar
- TÃ¼rkÃ§e cevap Ã¼retimi iÃ§in yÃ¶nlendirir
- Kaynak belirtme kurallarÄ± tanÄ±mlar

---

# 3. DokÃ¼man Ä°ÅŸleme ve VektÃ¶r VeritabanÄ±

## DokÃ¼man Ä°ÅŸleme Pipeline

```mermaid
flowchart LR
    A["ğŸ“„ DokÃ¼man"] --> B["ğŸ“– Metin Ã‡Ä±karma"]
    B --> C["âœ‚ï¸ Chunking"]
    C --> D["ğŸ”¢ Embedding"]
    D --> E["ğŸ’¾ ChromaDB"]
```

## Desteklenen Formatlar

| Format | KÃ¼tÃ¼phane | Ã–zellikler |
|--------|-----------|------------|
| PDF | PyMuPDF | HÄ±zlÄ±, gÃ¼venilir |
| TXT/MD | Built-in | UTF-8 desteÄŸi |
| DOCX | python-docx | Paragraf bazlÄ± |

## Chunking Stratejisi

```python
# Parametreler
chunk_size = 500 karakter
chunk_overlap = 50 karakter
```

**Neden Overlap?**
- CÃ¼mle bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korur
- BaÄŸlam kaybÄ±nÄ± Ã¶nler

## VektÃ¶r VeritabanÄ±: ChromaDB

**Tercih Nedenleri:**
- Python-native, ek servis gerektirmez
- KalÄ±cÄ± depolama (persist)
- Cosine similarity desteÄŸi

## Embedding Modeli

**all-MiniLM-L6-v2** (Sentence-Transformers)
- 384 boyutlu vektÃ¶rler
- HÄ±zlÄ± inference
- Ã‡ok dilli destek

---

# 4. API / Servis KatmanÄ±

## Teknoloji: FastAPI

**Neden FastAPI?**
- âš¡ Async/await desteÄŸi
- ğŸ“š Otomatik OpenAPI dokÃ¼mantasyonu
- âœ… Pydantic ile tip gÃ¼venliÄŸi
- ğŸ§ª Kolay test edilebilirlik

## Endpoint'ler

| Endpoint | Metod | AÃ§Ä±klama |
|----------|-------|----------|
| `/api/health` | GET | Servis durumu |
| `/api/ask` | POST | Soru-cevap |
| `/api/upload` | POST | DokÃ¼man yÃ¼kleme |
| `/api/documents` | GET | DokÃ¼man listesi |

## Request/Response AkÄ±ÅŸÄ±

```mermaid
sequenceDiagram
    participant User
    participant API
    participant VectorDB
    participant LLM

    User->>API: POST /ask {question}
    API->>VectorDB: search(question)
    VectorDB-->>API: relevant_chunks
    API->>LLM: generate(question, context)
    LLM-->>API: answer
    API-->>User: {answer, sources}
```

## Hata YÃ¶netimi

- 400: GeÃ§ersiz istek (boÅŸ soru, yanlÄ±ÅŸ format)
- 404: Kaynak bulunamadÄ±
- 503: LLM servisi kullanÄ±lamÄ±yor
- 500: Beklenmeyen sunucu hatasÄ±

---

# 5. Genel Ã‡alÄ±ÅŸma Mimarisi

```mermaid
flowchart TB
    subgraph Client["ğŸ–¥ï¸ Ä°stemci"]
        UI["Web ArayÃ¼zÃ¼<br/>(HTML/CSS/JS)"]
    end

    subgraph Backend["âš¡ FastAPI Backend"]
        Router["API Router"]
        
        subgraph Services["Servisler"]
            DocSvc["Document<br/>Service"]
            VecSvc["Vector<br/>Service"]
            LLMSvc["LLM<br/>Service"]
        end
    end

    subgraph Storage["ğŸ’¾ Depolama"]
        Files["DokÃ¼manlar<br/>(PDF/TXT/MD)"]
        Chroma["ChromaDB<br/>(VektÃ¶rler)"]
    end

    subgraph LLM["ğŸ¤– Yerel LLM"]
        Ollama["Ollama Server<br/>(llama3.2)"]
    end

    UI -->|"HTTP"| Router
    Router --> DocSvc
    Router --> VecSvc
    Router --> LLMSvc

    DocSvc --> Files
    VecSvc --> Chroma
    LLMSvc --> Ollama

    VecSvc -.->|"context"| LLMSvc
```

## Veri AkÄ±ÅŸÄ±

### DokÃ¼man YÃ¼kleme
1. KullanÄ±cÄ± dosya seÃ§er
2. API dosyayÄ± alÄ±r, doÄŸrular
3. DocumentService metni Ã§Ä±karÄ±r
4. Metin chunk'lara ayrÄ±lÄ±r
5. VectorService embedding oluÅŸturur
6. ChromaDB'ye kaydedilir

### Soru Sorma
1. KullanÄ±cÄ± soru yazar
2. Soru embedding'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
3. ChromaDB'de benzer chunk'lar aranÄ±r
4. En alakalÄ± chunk'lar seÃ§ilir
5. LLM'e soru + context gÃ¶nderilir
6. Cevap kullanÄ±cÄ±ya dÃ¶ndÃ¼rÃ¼lÃ¼r

---

# 6. LiteratÃ¼r AraÅŸtÄ±rmasÄ±

## Kaynaklar

### RAG YaklaÅŸÄ±mÄ±
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - Lewis et al., 2020
- [LangChain Documentation](https://python.langchain.com/)

### VektÃ¶r VeritabanlarÄ±
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [FAISS vs ChromaDB Comparison](https://medium.com/)

### Yerel LLM
- [Ollama Documentation](https://ollama.ai/)
- [LM Studio vs Ollama](https://github.com/)

## KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz

### VektÃ¶r VeritabanÄ± SeÃ§imi

| Ã–zellik | ChromaDB | FAISS | Pinecone |
|---------|----------|-------|----------|
| Kurulum | âœ… Kolay | âš ï¸ Orta | âš ï¸ Bulut |
| KalÄ±cÄ±lÄ±k | âœ… Var | âŒ Manuel | âœ… Var |
| Maliyet | âœ… Ãœcretsiz | âœ… Ãœcretsiz | âŒ Ãœcretli |
| Ã–lÃ§eklenebilirlik | âš ï¸ Orta | âœ… YÃ¼ksek | âœ… YÃ¼ksek |

**ChromaDB Tercihi**: Proje Ã¶lÃ§eÄŸinde yeterli, kurulumu kolay, Python-native.

### Embedding Modeli SeÃ§imi

| Model | Boyut | HÄ±z | Kalite |
|-------|-------|-----|--------|
| all-MiniLM-L6-v2 | 384 | âœ… HÄ±zlÄ± | âš ï¸ Ä°yi |
| all-mpnet-base-v2 | 768 | âš ï¸ Orta | âœ… YÃ¼ksek |
| OpenAI Ada | 1536 | âŒ API | âœ… YÃ¼ksek |

**MiniLM Tercihi**: HÄ±z/kalite dengesi, yerel Ã§alÄ±ÅŸma, yeterli performans.

## Ã–ÄŸrenilen Dersler

1. **Chunk boyutu kritik**: Ã‡ok kÃ¼Ã§Ã¼k â†’ baÄŸlam kaybÄ±, Ã§ok bÃ¼yÃ¼k â†’ gÃ¼rÃ¼ltÃ¼
2. **Overlap Ã¶nemli**: CÃ¼mle sÄ±nÄ±rlarÄ±nÄ± korumak iÃ§in gerekli
3. **Prompt mÃ¼hendisliÄŸi**: RAG iÃ§in Ã¶zel prompt tasarÄ±mÄ± ÅŸart
4. **Hata yÃ¶netimi**: LLM timeout'larÄ± ve baÄŸlantÄ± kopmalarÄ± iÃ§in hazÄ±rlÄ±klÄ± olunmalÄ±

---

# SonuÃ§

Bu proje, yerel LLM ve RAG yaklaÅŸÄ±mÄ±nÄ± bir araya getirerek:

âœ… **Gizlilik**: Veriler kullanÄ±cÄ±nÄ±n bilgisayarÄ±nda kalÄ±r  
âœ… **Maliyet**: Ãœcretsiz, aÃ§Ä±k kaynak araÃ§lar  
âœ… **Esneklik**: FarklÄ± modeller ve yapÄ±landÄ±rmalar  
âœ… **KullanÄ±labilirlik**: Web arayÃ¼zÃ¼ ile kolay eriÅŸim  

saÄŸlamaktadÄ±r.

---

**HazÄ±rlayan**: [Ä°sim]  
**Tarih**: Åubat 2026
