# Yerel LLM ile DokÃ¼man Soru-Cevap Servisi

Yerel Ã§alÄ±ÅŸan bir LLM (Ollama) kullanarak dokÃ¼manlarÄ±nÄ±z Ã¼zerinden soru-cevap yapmanÄ±zÄ± saÄŸlayan RAG (Retrieval-Augmented Generation) tabanlÄ± bir servistir.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ¯ Ã–zellikler

- ğŸ“„ **Ã‡oklu DokÃ¼man DesteÄŸi**: PDF, TXT, Markdown ve DOCX dosyalarÄ±nÄ± iÅŸler
- ğŸ” **Semantik Arama**: Sentence-Transformers ile embedding tabanlÄ± benzerlik aramasÄ±
- ğŸ¤– **Yerel LLM**: Ollama ile tamamen yerel Ã§alÄ±ÅŸÄ±r, verileriniz sizde kalÄ±r
- âš¡ **HÄ±zlÄ± API**: FastAPI ile async, yÃ¼ksek performanslÄ± REST API
- ğŸ¨ **Modern ArayÃ¼z**: KullanÄ±cÄ± dostu web arayÃ¼zÃ¼

## ğŸ“‹ Gereksinimler

- Python 3.10+
- [Ollama](https://ollama.ai/) kurulu ve Ã§alÄ±ÅŸÄ±r durumda
- 8GB+ RAM (embedding modeli iÃ§in)

## ğŸš€ Kurulum

### 1. Ollama Kurulumu

[Ollama](https://ollama.ai/) sitesinden iÅŸletim sisteminize uygun sÃ¼rÃ¼mÃ¼ indirip kurun.

Model indirme:
```bash
ollama pull llama3.1:8b
```

### 2. Proje Kurulumu

```bash
# Repo'yu klonlayÄ±n
git clone https://github.com/yourusername/local-rag-service.git
cd local-rag-service

# Sanal ortam oluÅŸturun
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin
pip install -r requirements.txt
```

### 3. Servisi BaÅŸlatma

```bash
# Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
ollama serve

# Yeni terminalde servisi baÅŸlatÄ±n
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Servis `http://localhost:8000` adresinde Ã§alÄ±ÅŸmaya baÅŸlayacaktÄ±r.

## ğŸ“– KullanÄ±m

### Web ArayÃ¼zÃ¼

TarayÄ±cÄ±nÄ±zda `http://localhost:8000` adresine gidin:

1. Sol panelden dokÃ¼man yÃ¼kleyin (PDF, TXT, MD, DOCX)
2. Soru alanÄ±na sorunuzu yazÄ±n
3. "Sor" butonuna tÄ±klayÄ±n veya Enter'a basÄ±n
4. CevabÄ± ve kaynak dokÃ¼manlarÄ± gÃ¶rÃ¼ntÃ¼leyin

### API KullanÄ±mÄ±

#### SaÄŸlÄ±k KontrolÃ¼
```bash
curl http://localhost:8000/api/health
```

#### DokÃ¼man YÃ¼kleme
```bash
curl -X POST http://localhost:8000/api/upload \
  -F "file=@/path/to/document.pdf"
```

#### Soru Sorma
```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "DokÃ¼man ne hakkÄ±nda?"}'
```

### API DokÃ¼mantasyonu

Swagger UI: `http://localhost:8000/docs`
ReDoc: `http://localhost:8000/redoc`

## ğŸ—ï¸ Proje YapÄ±sÄ±

```
local-rag-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI ana uygulama
â”‚   â”œâ”€â”€ config.py            # YapÄ±landÄ±rma
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py        # API endpoint'leri
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic modelleri
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_service.py   # Ollama entegrasyonu
â”‚       â”œâ”€â”€ document_service.py  # DokÃ¼man iÅŸleme
â”‚       â””â”€â”€ vector_service.py    # ChromaDB iÅŸlemleri
â”œâ”€â”€ tests/                   # Unit testler
â”œâ”€â”€ static/                  # Frontend dosyalarÄ±
â”œâ”€â”€ documents/               # YÃ¼klenen dokÃ¼manlar
â”œâ”€â”€ chroma_db/              # VektÃ¶r veritabanÄ±
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ YapÄ±landÄ±rma

Ortam deÄŸiÅŸkenleri veya `.env` dosyasÄ± ile yapÄ±landÄ±rabilirsiniz:

| DeÄŸiÅŸken | VarsayÄ±lan | AÃ§Ä±klama |
|----------|------------|----------|
| `OLLAMA_BASE_URL` | http://localhost:11434 | Ollama sunucu adresi |
| `OLLAMA_MODEL` | llama3.1:8b | KullanÄ±lacak LLM modeli |
| `EMBEDDING_MODEL` | all-MiniLM-L6-v2 | Embedding modeli |
| `CHUNK_SIZE` | 500 | DokÃ¼man parÃ§a boyutu |
| `TOP_K_RESULTS` | 3 | Arama sonuÃ§ sayÄ±sÄ± |

## ğŸ§ª Testler

```bash
# TÃ¼m testleri Ã§alÄ±ÅŸtÄ±r
pytest tests/ -v

# Coverage raporu ile
pytest tests/ -v --cov=app --cov-report=html
```

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

| Teknoloji | Neden? |
|-----------|--------|
| **FastAPI** | Async desteÄŸi, otomatik OpenAPI dokÃ¼mantasyonu, tip gÃ¼venliÄŸi |
| **Ollama** | Kolay kurulum, OpenAI uyumlu API, geniÅŸ model desteÄŸi |
| **ChromaDB** | Hafif, Python-native, kalÄ±cÄ± depolama |
| **Sentence-Transformers** | Ãœcretsiz, yerel Ã§alÄ±ÅŸÄ±r, yÃ¼ksek kaliteli embedding'ler |
| **PyMuPDF** | HÄ±zlÄ± ve gÃ¼venilir PDF iÅŸleme |

## ğŸ“Š Mimari

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web UI     â”‚â”€â”€â”€â”€â–¶â”‚              FastAPI                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ Document â”‚ â”‚  Vector  â”‚ â”‚   LLM    â”‚ â”‚
                    â”‚  â”‚ Service  â”‚ â”‚  Service â”‚ â”‚  Service â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚            â”‚            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ Documents â”‚ â”‚  ChromaDB  â”‚ â”‚ Ollama  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Lisans

MIT License - Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

## ğŸ¤ KatkÄ±da Bulunma

1. Fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit edin (`git commit -m 'Add amazing feature'`)
4. Push edin (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n
